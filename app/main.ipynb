{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import yaml\n",
    "import gc\n",
    "\n",
    "\n",
    "from data_loader import *\n",
    "from utils import *\n",
    "from nn import *\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams.update({\n",
    "    'axes.facecolor': '#1e1e1e',\n",
    "    'figure.facecolor': '#1e1e1e',\n",
    "    'axes.edgecolor': 'white',\n",
    "    'axes.labelcolor': 'white',\n",
    "    'xtick.color': 'white',\n",
    "    'ytick.color': 'white',\n",
    "    'text.color': 'white',\n",
    "    'axes.grid': True,\n",
    "    'grid.color': 'gray'\n",
    "})\n",
    "\n",
    "pd.set_option(\"display.max_column\",None)\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def auto_reload():\n",
    "    %load_ext autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\",'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "home_dir = config['HOME_DIRECTORY']\n",
    "home_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model File Configs:\n",
    "# Every Run of the notebook is logged in to a submodel folder for it\n",
    "\n",
    "############################################################################\n",
    "# Please set output path to the project directory where it is uncompressed #\n",
    "############################################################################\n",
    "\n",
    "project_path = home_dir + \"/outputs/models/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_path = \"\"\n",
    "\n",
    "submodel_name = \"\"\n",
    "\n",
    "encoding_path = \"\"\n",
    "feature_report_path = \"\"\n",
    "def create_submodel(model_name:str):\n",
    "    author = \"EJ\"\n",
    "    global submodel_name\n",
    "    submodel_name = datetime.now().strftime(\"%d_%H_%M\") + \"_\"+model_name\n",
    "    global output_path\n",
    "    output_path = project_path+submodel_name\n",
    "    global encoding_path\n",
    "    encoding_path = output_path+\"/encodings/\"\n",
    "    global feature_report_path\n",
    "    feature_report_path = output_path+\"/feature_report/\"\n",
    "    os.mkdir(output_path)\n",
    "    os.mkdir(encoding_path)\n",
    "    os.mkdir(feature_report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submodel(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Football:\n",
    "https://www.api-football.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PYTHON LOGO](https://www.api-football.com/public/img/news/archi-beta.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues_dat = get_leagues(home_dir +\"/data/Leagues/leagues.parquet\")\n",
    "leagues_dat[['league_id','league_name','country_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leagues subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "major_leagues = [\"Premier League\",\"La Liga\",\"Serie A\",\"Bundesliga\",\"Eredivisie\",\"Ligue 1\"]\n",
    "major_countries = [\"England\",\"Spain\",\"Italy\",\"Germany\",\"Netherlands\",\"France\",\"Brazil\"]\n",
    "teams = [\"Liverpool\",\"Wolves\"] # teams to pull players data of\n",
    "seasons = [2022,2021,2023,2024] # seasons to pull players and teams stats of\n",
    "\n",
    "\n",
    "\n",
    "leagues_subset = leagues_dat[leagues_dat.league_name.isin(major_leagues) & leagues_dat.country_name.isin(major_countries)] # league ID to pull from, current values: {39:premier league}, Add to dictionary as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All fixtures data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures_dir = home_dir + \"/data/Fixtures\"\n",
    "\n",
    "complete_data = pd.DataFrame()\n",
    "for file in os.listdir(fixtures_dir):    \n",
    "    dat = pd.read_parquet(os.path.join(fixtures_dir,file))\n",
    "    complete_data = pd.concat([complete_data,dat],axis = 0)\n",
    "\n",
    "complete_data = complete_data.reset_index()\n",
    "complete_data.drop(columns = ['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data checks\n",
    "complete_data['passes_accuracy'] = complete_data['passes_accuracy'].astype(\"float64\")\n",
    "\n",
    "# Targets\n",
    "complete_data['outcome_num'] = pd.Categorical(complete_data.outcome).codes\n",
    "\n",
    "complete_data['win'] = np.where(complete_data.outcome.str.lower() == 'win', 1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dictionary that contains all information about the features    \n",
    "dat_dict = find_data_types(complete_data,config['OUTCOME_COLS'] + ['outcome_num','outcome'])\n",
    "dat_dict = pd.DataFrame(list(dat_dict.items()),columns =['feature','type'])\n",
    "\n",
    "# differentiate modeling features\n",
    "non_modeling_features = config['FIXTURE_COLS'] + config['OUTCOME_COLS'] + config['MISC_COLS'] + ['outcome_num']\n",
    "dat_dict['modeling_feature'] = np.where(dat_dict['feature'].isin(non_modeling_features),0,1)\n",
    "dat_dict['encoded'] = 0\n",
    "\n",
    "print(dat_dict['type'].value_counts())\n",
    "dat_dict.reset_index(drop= True)\n",
    "dat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encode Features\n",
    "dat_dict = create_data_index(complete_data,dat_dict,'target',encoding_path)\n",
    "dat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick knn clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# who is ekitike most similar to \n",
    "\n",
    "stiker_stats = complete_data[['games_position','player_name'] + attacking_stat_cols].fillna(0)[complete_data[['games_position','player_name'] + attacking_stat_cols].fillna(0).games_position.isin(['F'])].groupby(['player_name']).agg(n_games = ('player_name','size'),\n",
    "                                                                          avg_dribble_success = ('dribble_success_rate','mean'),\n",
    "                                                                          std_dribble_success = ('dribble_success_rate','std'),\n",
    "                                                                          avg_target_shot_cr = ('target_shot_conversion_perc','mean'),\n",
    "                                                                          std_target_shot_cr = ('target_shot_conversion_perc','std'),\n",
    "                                                                          avg_duels_cr = ('duels_won_perc','mean'),\n",
    "                                                                          std_duels_cr = ('duels_won_perc','std'),\n",
    "                                                                          avg_pass_accuracy = ('pass_accuracy_perc','mean'),\n",
    "                                                                          std_pass_accuracy = ('pass_accuracy_perc','std')\n",
    "                                                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stiker_stats = stiker_stats[stiker_stats.n_games > 30]\n",
    "stiker_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stiker_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stiker_stats.sort_values('avg_dribble_success',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among 18 players, size 3 uniform clusters\n",
    "km = KMeans(10,random_state= 5)\n",
    "stiker_stats['cluster_dribble'] = km.fit_predict(stiker_stats[['avg_dribble_success']])\n",
    "\n",
    "km = KMeans(10,random_state= 5)\n",
    "stiker_stats['cluster_shot_cr'] = km.fit_predict(stiker_stats[['avg_target_shot_cr']])\n",
    "\n",
    "km = KMeans(10,random_state= 5)\n",
    "stiker_stats['cluster_duels'] = km.fit_predict(stiker_stats[['avg_duels_cr']])\n",
    "\n",
    "km = KMeans(10,random_state= 5)\n",
    "stiker_stats['cluster_pass'] = km.fit_predict(stiker_stats[['avg_pass_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check by clusters:\n",
    "stiker_stats.sort_values([\"avg_target_shot_cr\",\"cluster_shot_cr\"],ascending= [0,0])[[\"avg_target_shot_cr\",\"std_target_shot_cr\",\"cluster_shot_cr\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stiker_stats.sort_values([\"avg_dribble_success\",\"cluster_dribble\"],ascending= [0,0])[[\"avg_dribble_success\",\"std_dribble_success\",\"cluster_dribble\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stiker_stats.sort_values([\"avg_duels_cr\",\"cluster_duels\"],ascending= [0,0])[[\"avg_duels_cr\",\"std_duels_cr\",\"cluster_duels\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_query = 'games_position.isin([\"F\"])'\n",
    "\n",
    "\n",
    "# Stat to look at:\n",
    "stat = 'target_shot_conversion_perc'\n",
    "\n",
    "# configs \n",
    "min_appearance = 40\n",
    "\n",
    "dribble_dat_g = complete_data[complete_data.games_position.isin(['F'])].reset_index().groupby(\"player_name\").agg(n_apps = (\"player_name\",\"size\"),stat = (stat,\"median\")).reset_index()\n",
    "dribble_dat_g = dribble_dat_g[dribble_dat_g.n_apps >= min_appearance]\n",
    "dribble_dat_g['rank'] = dribble_dat_g[\"stat\"].fillna(0).rank(ascending= False,method = 'dense')\n",
    "dribble_dat_g.sort_values(\"rank\",inplace = True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "# Plot correctly, no comma here\n",
    "sns.boxplot(\n",
    "    data=dribble_dat[dribble_dat.player_name.isin(dribble_dat_g[dribble_dat_g['rank'] < 15]['player_name'])],\n",
    "    x=\"player_name\",\n",
    "    y=stat,\n",
    "    order=dribble_dat_g[dribble_dat_g['rank'] < 15]['player_name'],\n",
    "    ax=ax,\n",
    "    \n",
    ")\n",
    "\n",
    "# Now this works correctly on `ax`\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_title(f\"Stat: {stat}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data[['outcome_num','outcome']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(nrows=1, ncols = 1, figsize = (20,10))\n",
    "fig = sns.heatmap(complete_data.query(filter_query)[config['ATTACK_COLS'] + ['team_goals_scored','team_non_penalty_goals_scored','team_goals_conceded']].corr(),cmap = 'coolwarm')\n",
    "fig.set_xticklabels(fig.get_xticklabels(),rotation = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial multiclass model:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_data[list(set(config['DEFENSE_COLS'] + config['PASSING_COLS'] + config['ATTACK_COLS'])) + ['outcome_num']].drop(columns = 'outcome_num'),complete_data[list(set(config['DEFENSE_COLS'] + config['PASSING_COLS'] + config['ATTACK_COLS'])) + ['outcome_num']][['outcome_num']],stratify=nn_dat[['outcome_num']],random_state=33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = NNDataFromPd(X_train.fillna(0), y_train.outcome_num, dat_dict)\n",
    "train_loader = DataLoader(dat, batch_size = 128,shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.X_numeric_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = y_train.iloc[:,0].nunique()\n",
    "model = MultiClassModel(n_features,n_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    for X_numeric_batch, X_categoric_batch, y_batch in train_loader:\n",
    "        \n",
    "        pred = model.forward(X_numeric_batch)\n",
    "        \n",
    "        loss = criterion(pred,y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['passes_accuracy'] = X_test['passes_accuracy'].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sets\n",
    "\n",
    "test_dat = NNDataFromPd(X_test,y_test,dat_dict)\n",
    "test_loader = DataLoader(test_dat,batch_size= X_test.shape[0],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_numeric_batch, X_categoric_batch, y_batch in test_loader:\n",
    "        output = model(X_numeric_batch)\n",
    "        pred_class = torch.argmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model:\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_data[complete_data.games_position == 'F'][list(set(config['DEFENSE_COLS'] + config['PASSING_COLS'] + config['ATTACK_COLS'])) + ['win']].drop(columns = 'win'),\n",
    "                                                    complete_data[complete_data.games_position == 'F']['win'],\n",
    "                                                    stratify=complete_data['win'],\n",
    "                                                    random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = NNDataFromPd(X_train,y_train,dat_dict)\n",
    "train_loader = DataLoader(train_dat,batch_size= 128,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "model = LogisticNNModelComplex(n_features)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_numeric, X_categoric, y in train_loader:\n",
    "\n",
    "        pred = model(X_numeric)\n",
    "\n",
    "        loss = criterion(pred,y.unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch},  Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validations import *\n",
    "\n",
    "\n",
    "test_dat = NNDataFromPd(X_test,y_test,dat_dict)\n",
    "test_loader = DataLoader(test_dat,batch_size= X_test.shape[0],shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_numeric_batch, X_categoric_batch, y_batch in test_loader:\n",
    "        output = model(X_numeric_batch)\n",
    "        pred_proba = torch.softmax(output,dim =1)\n",
    "        pred_class = torch.argmax(output, dim = 1)\n",
    "\n",
    "\n",
    "discrete_evaluations(y_test,pred_class,pred_proba.squeeze(1),classification_type=\"Binary\",model_path= output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
